{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e1d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model class with init and forward functions\n",
    "import csv\n",
    "import os, random, sys, copy\n",
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class sarcasmModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, lstm_hidden_size=50, num_lstm_layers=1, bidirectional=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "        self.lstm = nn.LSTM(input_size = embedding_matrix.shape[1],\n",
    "                            hidden_size = lstm_hidden_size,\n",
    "                            num_layers = num_lstm_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            batch_first = True)\n",
    "        \n",
    "        self.hidden_1 = nn.Linear(lstm_hidden_size * 2, lstm_hidden_size)\n",
    "        self.hidden_2 = nn.Linear(lstm_hidden_size, 1)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input_batch, input_lengths):\n",
    "        \n",
    "        embedded_input = self.embedding(input_batch)\n",
    "        \n",
    "        packed_input = pack_padded_sequence(embedded_input, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)                                                                       # See docs linked below for description of hn.shape\n",
    "        \n",
    "        hn_view = hn.view(self.lstm.num_layers, self.num_directions, input_batch.shape[0], self.lstm.hidden_size)               # Reshape hn for clarity -- first dimension now represents each layer (total set by num_lstm_layers)\n",
    "        \n",
    "        hn_view_last_layer = hn_view[-1]                                                                                        # Taking the last layer for our final LSTM output\n",
    "        \n",
    "        hn_cat = torch.cat([hn_view_last_layer[-2, :, :], hn_view_last_layer[-1, :, :]], dim=1)                                 # Each layer has two directions. We want to use both of these vectors, so concatenate them\n",
    "        \n",
    "        hid = self.relu(self.hidden_1(hn_cat))\n",
    "        \n",
    "        output = self.hidden_2(hid)\n",
    "        \n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
